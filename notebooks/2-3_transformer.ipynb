{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90ead97-2f13-4246-9ba4-59d6b928886b",
   "metadata": {},
   "source": [
    "# PyTorch による Transformer の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c9117-f5c4-4a3a-8cd9-06329c8b8738",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/py-img-gen/python-image-generation/blob/main/notebooks/2-3_transformer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f70d06-6732-485f-a789-5fc21f0bc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq py-img-gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003aa791",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16480349-0578-439f-90bc-b86095375457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb86bf-3ef1-44fa-9611-5e1b0cfcb452",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Transformer の実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807f9dc2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        num_encoders: int = 6,\n",
    "        num_decoders: int = 6,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            d_model, num_heads, num_encoders\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            d_model, num_heads, num_decoders\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "        dec_out = self.decoder(\n",
    "            tgt, enc_out, src_mask, tgt_mask\n",
    "        )\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b76e08-16b4-4621-a120-bb7fb2e52382",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Transformer Encoder の実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be2f9de-c01f-4049-bac1-bf3a2cef14ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        num_encoders: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            EncoderLayer(d_model, num_heads)\n",
    "            for _ in range(num_encoders)\n",
    "        ]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(\n",
    "        self, src: torch.Tensor, src_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, src_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15975519-f972-455e-af39-b970ef96444f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Transformer Decoder の実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80aee3ac-e28e-49f2-bbc2-0b185c4b12d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        num_decoders: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            DecoderLayer(d_model, num_heads)\n",
    "            for _ in range(num_decoders)\n",
    "        ]\n",
    "        self.dec_layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        enc: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor,\n",
    "        enc_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        output = tgt\n",
    "        for layer in self.dec_layers:\n",
    "            output = layer(output, enc, tgt_mask, enc_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cbbc9-a014-432a-be57-a103150d810d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Transformer Encoder レイヤーの実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0f4579-8d09-4542-8807-bd74639567e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int = 2048,\n",
    "        dropout: float = 0.3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.mha = MultiheadAttention(\n",
    "            d_model, num_heads, dropout\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(\n",
    "        self, src: torch.Tensor, src_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        x = src\n",
    "        x = x * self.mha(q=x, k=x, v=x, mask=src_mask)\n",
    "        x = self.attn_norm(x)\n",
    "        x = x + self.ffn(x)\n",
    "        x = self.ffn_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485bc5b1-ab38-4026-8406-4a8f5b1d0db7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Multi-head Attention の実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6754ba14-1fe4-4ac7-8982-cf941edc826d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model: int, num_heads: int, dropout: float\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        attn_output_size = self.d_model // self.num_heads\n",
    "        attns = [\n",
    "            SelfAttention(d_model, attn_output_size)\n",
    "            for _ in range(self.num_heads)\n",
    "        ]\n",
    "        self.attentions = nn.ModuleList(attns)\n",
    "        self.output = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        x = torch.cat(\n",
    "            [\n",
    "                layer(q, k, v, mask)\n",
    "                for layer in self.attentions\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce0c5b-cad0-4c2a-bd7b-7c19b7cb3ee5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Self-Attention の実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e417317b-e6f0-45a2-add3-685c86f0057d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        output_size: int,\n",
    "        dropout: float = 0.3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(d_model, output_size)\n",
    "        self.k = nn.Linear(d_model, output_size)\n",
    "        self.v = nn.Linear(d_model, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        bs = q.size(dim=0)\n",
    "        tgt_len = q.size(dim=1)\n",
    "        seq_len = k.size(dim=1)\n",
    "        q, k, v = self.q(q), self.k(v), self.v(v)\n",
    "        dim_k = k.size(dim=-1)\n",
    "        scores = torch.bmm(\n",
    "            q, k.transpose(1, 2)\n",
    "        ) / torch.sqrt(dim_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, :]\n",
    "            expanded_mask = mask.expand(\n",
    "                bs, tgt_len, seq_len\n",
    "            )\n",
    "            scores = scores.masked_fill(\n",
    "                expanded_mask == 0, -float(\"Inf\")\n",
    "            )\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        outputs = torch.bmm(weights, v)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a6683-a725-4c1d-b8a5-b4493a74d310",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Transformer Decoder レイヤーの実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "566a9c59-4631-47b2-a956-c1ef09a4b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        d_ff: int = 2048,\n",
    "        dropout: float = 0.3,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.masked_mha = MultiheadAttention(\n",
    "            d_model, num_heads, dropout\n",
    "        )\n",
    "        self.mha = MultiheadAttention(\n",
    "            d_model, num_heads, dropout\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.masked_mha_norm = nn.LayerNorm(d_model)\n",
    "        self.mha_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tgt: torch.Tensor,\n",
    "        enc: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor,\n",
    "        enc_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        x = tgt\n",
    "        x = x + self.masked_mha(\n",
    "            q=x, k=x, v=x, mask=tgt_mask\n",
    "        )\n",
    "        x = self.masked_mha_norm(x)\n",
    "        x = x + self.mha(q=x, k=enc, v=enc, mask=enc_mask)\n",
    "        x = self.mha_norm(x)\n",
    "        x = x + self.ffn(x)\n",
    "        x = self.ffn_norm(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
