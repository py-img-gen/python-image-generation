{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5d9fe4-0e2d-4659-a748-7f358d576d1d",
   "metadata": {},
   "source": [
    "https://github.com/JeongJiHeon/ScoreDiffusionModel/blob/main/DDPM/DDPM_MNIST.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca51768",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12888f0c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a directory: /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "current_dir = pathlib.Path.cwd()\n",
    "project_dir = current_dir / \"data\" / \"ddpm\"\n",
    "project_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Created a directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "401fba2d-cf7c-4a2c-a848-1421fc8a2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5e9912-068c-4554-84da-0dc0dac2d192",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainDDPMConfig(output_dir=PosixPath('/root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm'), seed=42, batch_size=256, num_epochs=10, num_timesteps=500, lr=0.0001, num_workers=4, beta_1=0.0001, beta_T=0.02, eta_ddim=0.0)\n"
     ]
    }
   ],
   "source": [
    "from py_img_gen.trainers import TrainDDPMConfig\n",
    "\n",
    "train_config = TrainDDPMConfig(output_dir=project_dir)\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79935be8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalConfig(eval_epoch=1, num_generate_images=16, num_grid_rows=4, num_grid_cols=4)\n"
     ]
    }
   ],
   "source": [
    "from py_img_gen.trainers import EvalConfig\n",
    "\n",
    "eval_config = EvalConfig()\n",
    "print(eval_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8254374d-ba01-45c4-970c-248ad23db772",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPMModelConfig(sample_size=32, in_channels=1, out_channels=1, block_out_channels=(64, 128, 256, 512), layers_per_block=3, down_block_types=('DownBlock2D', 'DownBlock2D', 'DownBlock2D', 'DownBlock2D'), up_block_types=('UpBlock2D', 'UpBlock2D', 'UpBlock2D', 'UpBlock2D'))\n"
     ]
    }
   ],
   "source": [
    "from py_img_gen.trainers import DDPMModelConfig\n",
    "\n",
    "model_config = DDPMModelConfig()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958d5e62-eb06-4214-aa68-16c0367a9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(seed=train_config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b52f72d-0496-4918-82f4-29e7dd6733d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "unet = UNet2DModel(\n",
    "    **asdict(model_config),\n",
    ")\n",
    "unet = unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a72529-e016-47fb-94f9-a90d67e5acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=train_config.num_timesteps,\n",
    "    beta_start=train_config.beta_1,\n",
    "    beta_end=train_config.beta_T,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654ae478",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(\n",
    "    unet.parameters(), lr=train_config.lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d71422-50ef-45c2-b122-0d14268b1950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from py_img_gen.trainers import (\n",
    "    get_simple_resize_transforms,\n",
    ")\n",
    "\n",
    "transform = get_simple_resize_transforms(\n",
    "    sample_size=model_config.sample_size\n",
    ")\n",
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df602ad0-aab6-4f3b-a6b2-1876de893ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:02<00:00, 4.79MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw/train-images-idx3-ubyte.gz to /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 174kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw/train-labels-idx1-ubyte.gz to /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:01<00:00, 1.54MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 4.86MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/ghq/github.com/py-img-gen/python-image-generation.bk/notebooks/data/ddpm/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\n",
    "    root=project_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=train_config.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=train_config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d577156-495e-4dff-a08d-1b58f94c8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from diffusers.utils import make_image_grid\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train_iteration(\n",
    "    train_config: TrainDDPMConfig,\n",
    "    unet: UNet2DModel,\n",
    "    noise_scheduler: DDPMScheduler,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    data_loader: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    with tqdm(\n",
    "        total=len(data_loader),\n",
    "        desc=\"Iteration\",\n",
    "        leave=False,\n",
    "    ) as pbar:\n",
    "        for x, _ in data_loader:\n",
    "            bsz = x.shape[0]\n",
    "            x = x.to(device)\n",
    "\n",
    "            t = torch.randint(\n",
    "                low=0,\n",
    "                high=train_config.num_timesteps,\n",
    "                size=(bsz,),\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            z = torch.randn_like(x)\n",
    "            x_noisy = noise_scheduler.add_noise(x, z, t)\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            z_pred = unet(x_noisy, t).sample\n",
    "            loss = F.mse_loss(z_pred, z)\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            pbar.set_postfix(loss=loss.detach().item())\n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e799de0-5416-4a0b-ae2d-5733d11c51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_config: TrainDDPMConfig,\n",
    "    eval_config: EvalConfig,\n",
    "    unet: UNet2DModel,\n",
    "    noise_scheduler: DDPMScheduler,\n",
    "    optim: torch.optim.Optimizer,\n",
    "    data_loader: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    # UNet を訓練モードに設定\n",
    "    unet.train()  # type: ignore[attr-defined]\n",
    "\n",
    "    for epoch in tqdm(\n",
    "        range(train_config.num_epochs), desc=\"Epoch\"\n",
    "    ):\n",
    "        train_iteration(\n",
    "            train_config=train_config,\n",
    "            unet=unet,\n",
    "            noise_scheduler=noise_scheduler,\n",
    "            optim=optim,\n",
    "            data_loader=data_loader,\n",
    "            device=device,\n",
    "        )\n",
    "        images = inference(\n",
    "            unet=unet,\n",
    "            noise_scheduler=noise_scheduler,\n",
    "            train_config=dataclasses.replace(\n",
    "                train_config,\n",
    "                batch_size=eval_config.num_generate_images,\n",
    "            ),\n",
    "        )\n",
    "        image = make_image_grid(\n",
    "            images=images,  # type: ignore\n",
    "            rows=eval_config.num_grid_rows,\n",
    "            cols=eval_config.num_grid_cols,\n",
    "        )\n",
    "        image.save(project_dir / f\"{epoch=}.png\")\n",
    "        image.save(project_dir / \"validation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "225ef56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "from diffusers.schedulers.scheduling_ddpm import (\n",
    "    DDPMSchedulerOutput,\n",
    ")\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from PIL.Image import Image as PilImage\n",
    "from py_img_gen.utils import decode_images\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference(\n",
    "    unet: UNet2DModel,\n",
    "    noise_scheduler: DDPMScheduler,\n",
    "    train_config: TrainDDPMConfig,\n",
    "    only_final: bool = True,\n",
    ") -> Union[List[PilImage], List[List[PilImage]]]:\n",
    "    # UNet を評価モードに設定\n",
    "    unet.eval()  # type: ignore[attr-defined]\n",
    "    # ノイズスケジューラにタイムステップを設定\n",
    "    noise_scheduler.set_timesteps(\n",
    "        train_config.num_timesteps\n",
    "    )\n",
    "    # 再現性のために推論用の乱数生成器を設定\n",
    "    generator = torch.manual_seed(train_config.seed)\n",
    "\n",
    "    # ノイズの形状を設定してからランダムノイズを生成\n",
    "    x_shape = (\n",
    "        train_config.batch_size,\n",
    "        unet.config.in_channels,\n",
    "        unet.config.sample_size,\n",
    "        unet.config.sample_size,\n",
    "    )\n",
    "    x = randn_tensor(\n",
    "        x_shape, generator=generator, device=unet.device\n",
    "    )\n",
    "\n",
    "    # 逆拡散過程を実行\n",
    "    images, timesteps = [], noise_scheduler.timesteps\n",
    "    for t in tqdm(\n",
    "        timesteps, desc=\"Generating...\", leave=False\n",
    "    ):\n",
    "        # ノイズ `z` を予測し、`z_pred` として取得\n",
    "        z_pred = unet(x, t).sample\n",
    "\n",
    "        # 一つ前の状態を計算: x_{t} -> x_{t-1}\n",
    "        output = noise_scheduler.step(\n",
    "            model_output=z_pred,\n",
    "            timestep=t,  # type: ignore\n",
    "            sample=x,\n",
    "            generator=generator,\n",
    "        )\n",
    "        x = (\n",
    "            output.prev_sample\n",
    "            if isinstance(output, DDPMSchedulerOutput)\n",
    "            else output[0]\n",
    "        )\n",
    "        if not only_final:\n",
    "            images.append(decode_images(x))\n",
    "\n",
    "    return decode_images(x) if only_final else images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b893fc-a837-4090-b92c-14d720e2f538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd8ef5dfdae4728afc3c5ff9b9291d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf60b91c75742eca80c30ac18d76297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 46.12 MiB is free. Process 123618 has 71.50 GiB memory in use. Process 111217 has 7.59 GiB memory in use. Of the allocated memory 6.90 GiB is allocated by PyTorch, and 203.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_config, eval_config, unet, noise_scheduler, optim, data_loader, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m unet\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mrange\u001b[39m(train_config\u001b[38;5;241m.\u001b[39mnum_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m ):\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mtrain_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43munet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     images \u001b[38;5;241m=\u001b[39m inference(\n\u001b[1;32m     28\u001b[0m         unet\u001b[38;5;241m=\u001b[39munet,\n\u001b[1;32m     29\u001b[0m         noise_scheduler\u001b[38;5;241m=\u001b[39mnoise_scheduler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m         ),\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m     image \u001b[38;5;241m=\u001b[39m make_image_grid(\n\u001b[1;32m     36\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         rows\u001b[38;5;241m=\u001b[39meval_config\u001b[38;5;241m.\u001b[39mnum_grid_rows,\n\u001b[1;32m     38\u001b[0m         cols\u001b[38;5;241m=\u001b[39meval_config\u001b[38;5;241m.\u001b[39mnum_grid_cols,\n\u001b[1;32m     39\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mtrain_iteration\u001b[0;34m(train_config, unet, noise_scheduler, optim, data_loader, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m x_noisy \u001b[38;5;241m=\u001b[39m noise_scheduler\u001b[38;5;241m.\u001b[39madd_noise(x, z, t)\n\u001b[1;32m     33\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 35\u001b[0m z_pred \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(z_pred, z)\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d.py:336\u001b[0m, in \u001b[0;36mUNet2DModel.forward\u001b[0;34m(self, sample, timestep, class_labels, return_dict)\u001b[0m\n\u001b[1;32m    334\u001b[0m         sample, skip_sample \u001b[38;5;241m=\u001b[39m upsample_block(sample, res_samples, emb, skip_sample)\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mupsample_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# 6. post-process\u001b[39;00m\n\u001b[1;32m    339\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_norm_out(sample)\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2740\u001b[0m, in \u001b[0;36mUpBlock2D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, upsample_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2736\u001b[0m             hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m   2737\u001b[0m                 create_custom_forward(resnet), hidden_states, temb\n\u001b[1;32m   2738\u001b[0m             )\n\u001b[1;32m   2739\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2740\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2743\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/diffusers/models/resnet.py:369\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(hidden_states)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_shortcut \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_shortcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m (input_tensor \u001b[38;5;241m+\u001b[39m hidden_states) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_scale_factor\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_tensor\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ghq/github.com/py-img-gen/python-image-generation.bk/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 46.12 MiB is free. Process 123618 has 71.50 GiB memory in use. Process 111217 has 7.59 GiB memory in use. Of the allocated memory 6.90 GiB is allocated by PyTorch, and 203.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    train_config=train_config,\n",
    "    eval_config=eval_config,\n",
    "    unet=unet,\n",
    "    noise_scheduler=noise_scheduler,\n",
    "    optim=optim,\n",
    "    data_loader=data_loader,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af51f32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from py_img_gen import inferencers\n",
    "\n",
    "ani = inferencers.animation_inference(\n",
    "    train_config=train_config,\n",
    "    eval_config=eval_config,\n",
    "    unet=unet,\n",
    "    noise_scheduler=noise_scheduler,\n",
    ")\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f64e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "\n",
    "pipe = DDPMPipeline(unet=unet, scheduler=noise_scheduler)\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(\n",
    "    num_inference_steps=train_config.num_timesteps,\n",
    "    batch_size=eval_config.num_generate_images,\n",
    "    generator=torch.manual_seed(train_config.seed),\n",
    ")\n",
    "image = make_image_grid(\n",
    "    images=output.images,\n",
    "    rows=eval_config.num_grid_rows,\n",
    "    cols=eval_config.num_grid_cols,\n",
    ")\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
